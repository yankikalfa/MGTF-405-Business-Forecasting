\documentclass[12pt,letter]{article}
\usepackage[utf8]{inputenc}
\usepackage{preamble}
\usepackage{graphicx}
\usepackage{nicematrix}
\usepackage{longtable}
\usepackage{lscape}
\usepackage[doublespacing]{setspace}
\author{S. Yanki Kalfa\\
	Yanki.Kalfa@rady.ucsd.edu\\
University of California - San Diego\\
Rady School of Management}
\title{Stationarity, ADF, and AR Forecasting}
\begin{document}
\maketitle
\section{Introduction}
This note provides a short summary of the following topics:
\begin{enumerate}
\item Integrated Processes\\
\item ADF Tests\\
\item AR Forecasts
\end{enumerate}
Each of these topics are covered from a theoretical perspective. For an applied treatment of the topics please see my \href{https://github.com/yankikalfa/MGTF-405-Business-Forecasting}{GitHub} page.

\section{Integrated Processes}

\subsection{Definitions}
Before jumping directly to stationary processes and non-stationary processes, we need to define the characteristics of the series. The first moment, or the unconditional mean of a series is defined as below:$$ E[y_{t}]=\mu_{t}$$
$\mu_{t}$ is hardly ever observed in the data. The covariance between two element of the series is defined as: $$ E[(y_{t}-\mu_{t})- (y_{t-j}-\mu_{t-j})]= \gamma_{j,t}$$
The covariance of the two elements displaced by j units is basically the autocovariance of order j. We want to have a series where the mean nor the autocovariance depend on time. Meaning that the expected value of the mean and the expected value of the autocovariances should not depend on time. If the series is time independent, then it is called covariance stationary, also called stationary. If the series is stationary then we have:
\begin{center}
\begin{framed}
\begin{eqnarray}
E[y_{t}]&=&\mu\\
E[(y_{t}-\mu)(y_{t-j}-\mu)]&=&\gamma_{j}\\
\gamma_{0}=E[(y_{t}-\mu)(y_{t}-\mu)]=E[(y_{t}-\mu)^{2}]=var(y_{t})&=&\sigma^{2}
\end{eqnarray}	
\end{framed} 
\end{center}
In finite samples we have:
\begin{equation*}
\hat{\gamma_{k}}= \frac{1}{T}\cdot \sum^{T}_{t=1} (y_{t}-\bar{y})(y_{t-k}-\bar{y})
\end{equation*}
Another characteristic we need to take into consideration is the correlation between two elements of time series, also known as autocorrelation.
$$ \rho_{\tau}=\dfrac{\gamma_{\tau}}{var(y)}$$\\
	
$$\hat{\rho_{\tau}}= \dfrac{1/T \cdot \sum_{t=1}^{T}\cdot (y_{t}-\bar{y})(y_{t-\tau}-\bar{y})}{1/T \cdot \sum_{t=1}^{T} (y-{t}-\bar{y})^{2}}$$

\subsection{Stationary Processes}

The first stationary process that we will look at is the white noise. It is the simplest stationaty process. We define the white noise as the following:$$ y_{t}=\mu + \epsilon_{t} | \epsilon_{t} \sim N(0,\sigma^{2}) i.i.d.$$

\begin{center}
	\begin{framed}
		\begin{eqnarray}
		E[y_{t}]= \mu + E[\epsilon_{t}] &=& \mu\\
		E[(y_{t}-\mu)(y_{t}-\mu)] = E [(\epsilon_{t}) (\epsilon_{t})] &=&= \sigma^{2} = \gamma_{0}\\
		E[(y_{t}-\mu)(y_{t-j}-\mu)] = E[(\epsilon_{t})(\epsilon_{t-j})] &=& 0 = \gamma_{j} ; j\geq 1
		\end{eqnarray}
	\end{framed}
\end{center} 
We proved that the WN process is stationary because its mean and covariance are time independent.\\
One common stationary process is the Moving Average of order one (MA(1)). Defined below: $$ y_{t}= \mu + \epsilon_{t} + \theta \epsilon_{t-1} | \epsilon_{t} \sim N(0, \sigma^{2}) i.i.d.$$
Proof that MA(1) is stationary:
\begin{center}
	\begin{framed}
		\begin{eqnarray}
		E[y_{t}] = E[\mu] + E[\epsilon_{t}] + E[\epsilon_{t-1}] &=& \mu\\
		E[(y_{t}-\mu)^{2}]= var(\epsilon_{t}) + var(\theta \epsilon_{t-1}) &=& (1+\theta) \sigma^{2}\\
		E[(y_{t}-\mu)(y_{t-1}-\mu)]=E[(\epsilon+ \theta \epsilon_{t-1}) (\epsilon_{t-1}+ \theta \epsilon_{t-2})]&=&\theta \sigma^{2}= \gamma_{1}\\
		\gamma_{2}&=&0 
		\end{eqnarray}
It is important to note that shocks in an MA(1) process die out after 1 period.
	\end{framed}
\end{center} 
An AR(1) is defined as shown below:$$y_{t}= c + \phi y_{t-1} + \epsilon_{t} | \epsilon_{t} \sim N(0,\sigma^{2}) i.i.d.$$
We can express the AR(1) process with recursive substitution in the following form: $$ y_{t}= \sum_{i=0}^{k} \phi^{i} \cdot c + \phi^{k} \cdot y_{t-k} + \sum_{i=0}^{k} \phi^{i} \epsilon_{t-i}$$
\begin{center}
	\begin{framed}
		\begin{eqnarray}
		E[y_{t}] = E[c] + \phi E[y_{t-1}] + E[\epsilon_{t}] &=& \frac{c}{1-\phi}\\
		E[(y_{t}-\mu)^{2}]= var(c) + \phi var(y_{t-1}) + var(\epsilon_{t}) &=& \frac{\sigma^{2}}{1- \phi^{2}}
		\end{eqnarray}
	\end{framed}
\end{center}
		These prove that the mean and the variance of the AR(1) are time independent. Lastly, we need to prove that the autocovariance of the series is also time invariant. First let's consider the following:\\
			$$(y_{t} - \mu) = c+ \phi y_{t-1} + \epsilon_{t} - \frac{c}{1-\phi} $$\\
		$$(y_{t} - \mu) = \frac{c}{1-\phi} - \frac{c}{1-\phi} -(\phi \cdot \frac{c}{1-\phi}) + \phi y_{t-1} + \epsilon_{t}$$ \\
		$$(y_{t} - \mu) = (y_{t-1}-\mu + \epsilon_{t})$$
Keeping this in mind let's look at the autocovariance:
\begin{center}
	\begin{framed}
		\begin{eqnarray}
		E[(y_{t}-\mu)(y_{t-1}-\mu)] &=& E[(y_{t-1}-\mu + \epsilon_{t})(y_{t-1}-\mu)]\\
		E[(y_{t-1}-\mu)(y_{t-1}-\mu)]+E[\epsilon_{t}(y_{t-1}-\mu)] &=& E[(y_{t-1}-\mu)(y_{t-1}-\mu)] \\
		\gamma_{1}=\phi var(y_{t}) &=& \phi \cdot \frac{\sigma^{2}}{1- \phi^{2}}\\
		\gamma_{k}=\phi^{k} \cdot \frac{\sigma^{2}}{1- \phi^{2}}
		\end{eqnarray}
	\end{framed}
\end{center}
Looking at the above proof, we can now say that the AR(1) process is stationary as long as the assumption $|\phi|<1$ holds. Another important thing to know about the AR(1) process is its autocorrelation function. The first order autocorrelation of the AR(1) is simply:
$$ \rho_{1}= \frac{\gamma_{1}}{\gamma_{0}}$$\\
And the $k^{th}$ autocorrelation is represented as:
$$ \rho_{k}= \frac{\gamma_{k}}{\gamma_{0}}$$\\
Of course, these only hold if the process is stationary.
\subsection{Non-Stationary - Integrated Processes}

When we think of non-stationary processes, we usually think of a random walk or a unit-root process. The difference between a stationary process and a non-stationary process is that the series/process is inherently unstable, meaning that the mean and variance are unbounded. Let $\phi=1$ then:\\
$$ E[y_{t}] = E[c] + \phi E[y_{t-1}] + E[\epsilon_{t}] = \frac{c}{1-\phi}= \infty$$
$$ E[(y_{t}-\mu)^{2}]= var(c) + \phi var(y_{t-1}) + var(\epsilon_{t}) = \frac{\sigma^{2}}{1- \phi^{2}}= \infty$$
$$ \gamma_{1}=\phi var(y_{t}) = \phi \cdot \frac{\sigma^{2}}{1- \phi^{2}}= \infty$$
This means that these values are time dependent and unbounded. If the variance and covariance of a process, t statistics will be unreliable. But this does not mean that we cannot work with a unit root process. If we can get the process to be stationary by first differencing then we say that the process is integrated of order 1 or I(1). We can show this in the following form:
$$ y_{t}= c+ \phi y_{t-1} + \epsilon_{t}$$
$$y_{t}-y_{t-1} = c + \phi y_{t-1} -y_{t-1} + \epsilon_{t}$$
$$\Delta y_{t} = c + (1-\phi) y_{t-1} + \epsilon_{t}$$
Now we can run tests on the $(1-\phi)$ coefficient, and this is called an I(1).



\section{Augmented Dickey-Fuller Tests}
The purpose of conducting a Dickey-Fuller (DF) test or am Augmented Dickey Fuller (ADF) test is to see if the series we are interested in has a unit-root. The ADF test transforms a regression in levels to a regression in changes. I will demonstrate it in the sections below.
\subsection{Dickey Fuller Test}
Let's take a simple AR(1):$$ x_{t}= \phi \cdot x_{t-1} + \epsilon_{t}$$

We can transform this to obtain the DF regression:
\begin{align*}
x_{t}=\phi \cdot x_{t-1} + \epsilon_{t}\\
x_{t}-x_{t-1}= \phi \cdot x_{t-1} - x_{t-1} + \epsilon_{t}\\
\Delta x_{t}= (\phi-1) \cdot x_{t-1} + \epsilon_{t}
\end{align*} 
we can call $(\phi-1) = \pi$.
\begin{align*}
\Delta x_{t}= \pi \cdot x_{t-1} + \epsilon_{t}
\end{align*}

The DF test's null hypothesis is:$$H_{0}: \pi=0$$ 
This is counter-intuitive, however, it is important to keep in mind that $\pi= (\phi -1)$ hence testing $\pi=0$ tests if $\phi=1$. \\
There is one issue with the DF test, it ignores serial correlation. Therefore, we usually run an Augmented Dickey Fuller test which includes differences of different orders.

\subsection{Augmented Dickey Fuller (ADF) Test}

Consider an AR(4): $$ x_{t}= \phi_{1} \cdot x_{t-1} + \phi_{2} \cdot x_{t-2} + \phi_{3} \cdot x_{t-3} + \phi_{4} \cdot x_{t-4}$$
We can transform this regression into an ADF(3) and obtain:$$ \Delta x_{t}= \pi \cdot x_{t-1} + \gamma_{1} \cdot \Delta x_{t-1} + \gamma_{2} \cdot x_{t-2} + \gamma_{3} \cdot x_{t-3}$$
where: $\pi=\phi_{1}+\phi_{2}+\phi_{3}+\phi_{4}-1$\\

By including the lags of the differences of the series $x_{t}$, the ADF solves the problem of serial correlation. The test has two different components, the first part is the lag selection and the second part is the unit root test on $\pi$.\\
Analytically, we can run the regression above by dropping the lag length one by one and look at the joint significance of the dropped lags. You can implement this by running these regressions and running a progress report, which will give you the likelihood ratio test for each model, and will also present AIC, BIC, and HQ information criterion. We select the smallest when looking at these information criteria. In practice, we can just run a unit root test in Python/R and select 3 lags. The unit root test includes AIC for lag selection. After looking at lag selection, we can look at unit-root tests. The null hypothesis is: $H_{0}: \pi=0$, if we fail to reject the null, then we say that the process is unit root, and is at least integrated of order 1  $\left(I(1)\right)$.   

\section{AR Forecasting}
 
We will start with a simple AR(1) process to showcase that as the forecast horizon grows the forecast itself will converge to the mean of the series. To make the demonstration simpler we will first subtract the mean from $y$ and then forecast.
\begin{align*}
(y_t-\mu) &= \phi(y_{t-1}-\mu) + \varepsilon_t\\
&\text{suppose now we want to forecast 3 steps ahead. We will iterate our equation forward}\\
(\hat{y_{t+1}}-\mu) &= \phi \left[ \phi(y_{t-1}-\mu) + \varepsilon_t\right] +\varepsilon_{t+1}\\
&=\phi^2 (y_t-\mu) +\phi \varepsilon_t +\varepsilon_{t+1}\\
(\hat{y_{t+2}}-\mu) &= \phi^3(y_t-\mu) + \phi^2 \varepsilon_t + \phi \varepsilon_{t+1} + \varepsilon_{t+2}\\
\hat{y_{t+3|t}} &= \mu + \phi^3(y_t-\mu) + \phi^2 \varepsilon_t + \phi \varepsilon_{t+1} + \varepsilon_{t+2}\\
E\left[y_{t+3}|y_t, y_{t-1}, \dots\right] & = \mu+ \phi^3 E[y_t- \mu]\\
\end{align*}
Now suppose that we roll this forward s times to get an s step-ahead forecast.
\begin{align*}
E\left[y_{t+s}|y_t, y_{t-1}, \dots\right] & = \mu+ \phi^s E[y_t- \mu]\\
\lim_{s\rightarrow \infty} E\left[y_{t+s}|y_t, y_{t-1}, \dots\right] & = \mu
\end{align*}
Hence we can see that the forecast is converging to the mean of the process.
\end{document} 
